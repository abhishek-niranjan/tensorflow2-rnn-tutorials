{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "7_GoogleNMT_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcM3Jya3tei5",
        "colab_type": "text"
      },
      "source": [
        "#### Following [Google’s Neural Machine Translation System: Bridging the Gap  between Human and Machine Translation]( https://arxiv.org/pdf/1609.08144.pdf )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from utils.dataset import NMTDataset\n",
        "from tensorflow.python.ops import math_ops"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQIr-vGDx97W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11985062-c93b-4218-a877-a7d9d6dbf59a"
      },
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64\n",
        "num_examples = 30000\n",
        "\n",
        "dataset_creator = NMTDataset('en-spa')\n",
        "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaE46NQJyCWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "da04c8d3-1208-4bcc-c9ae-fa678a319f54"
      },
      "source": [
        "print(\"Inpute Vocabulary Size: {}\".format(len(inp_lang.word_index)))\n",
        "print(\"Target Vocabulary Size: {}\".format(len(targ_lang.word_index)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inpute Vocabulary Size: 9414\n",
            "Target Vocabulary Size: 4935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DujAHBbYyFP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93a268ab-f087-4eae-fe92-96a65e8368d3"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qap4sWTJyHNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define some useful parameters for further use\n",
        "\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "embedding_dim = 128\n",
        "units = 1024\n",
        "steps_per_epoch = num_examples//BATCH_SIZE"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a6Z8kjKyK8c",
        "colab_type": "text"
      },
      "source": [
        "## [Google NMT Paper's Architecture](https://arxiv.org/pdf/1609.08144.pdf)\n",
        "\n",
        "We write the encoder-decoder modules as described in the original paper. \n",
        "<p align=\"center\">\n",
        "  <img src=\"https://slideplayer.com/slide/13133175/79/images/5/Model+Architecture+of+GNMT.jpg\" />\n",
        "</p>\n",
        "\n",
        "* There are residual connection from *3rd* layer onwards in both Encoder and Decoders.\n",
        "* The original paper uses a feedforward network for attention module. However, we'll use Bahdanau Attention. (In case you want to implement the feedforward attnetion module it's pretty straight forward.)\n",
        "  * Pass *h<sub>t-1</sub>* and *enc_outputs* to FFN layer, and return context vector of length *(BATCH_SIZE, units)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "##### GNMT Encoder - 8 layers, first layer is bidirectional\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.num_layers = 8\n",
        "    self.sub_layer = [None]*self.num_layers\n",
        "    self.hidden_hs = [None]*self.num_layers\n",
        "    self.hidden_cs = [None]*self.num_layers\n",
        "    self.num_residual_layers = self.num_layers - 2\n",
        "    self.num_bi_layers = 1\n",
        "    self.num_ui_layers = self.num_layers - self.num_bi_layers\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    #---- One Bidirectional Layer -----##\n",
        "    for i in range(self.num_bi_layers):\n",
        "      self.sub_layer[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units, return_sequences=True,                                                                    \n",
        "                                                  return_state=True, recurrent_initializer='glorot_uniform'))\n",
        "    \n",
        "    self.bi_h = tf.keras.layers.Dense(self.enc_units)\n",
        "    self.bi_c = tf.keras.layers.Dense(self.enc_units)\n",
        "    \n",
        "    ##----- Rest Unidirectional Layers ------##\n",
        "    for i in range(self.num_bi_layers, self.num_layers):\n",
        "      self.sub_layer[i] = tf.keras.layers.LSTM(self.enc_units, return_sequences=True, \n",
        "                                       return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    for i in range(self.num_bi_layers):\n",
        "      x, h_fw, c_fw, h_bw, c_bw = self.sub_layer[i](x)\n",
        "    \n",
        "    self.hidden_hs[i] = self.bi_h(tf.concat([h_fw, h_bw], axis=1))\n",
        "    self.hidden_cs[i] = self.bi_c(tf.concat([c_fw, c_bw], axis=1))\n",
        "    \n",
        "    \n",
        "    for i in range(self.num_bi_layers, self.num_layers):\n",
        "      prev_x = x\n",
        "      x, self.hidden_hs[i], self.hidden_cs[i] = self.sub_layer[i](x) \n",
        "      if(i !=1 ):\n",
        "        x = prev_x + x  ## Residual Connection\n",
        "        \n",
        "    return x, self.hidden_hs, self.hidden_cs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c8ee7023-e959-4ff9-faa0-e1006858f741"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "# sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print(len(sample_h))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h[0].shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c[0].shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "8\n",
            "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
            "Encoder c vector shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    ######## query : Decoder's Last Hidden State  #####\n",
        "    ######## values : Encoder Outputs             #####\n",
        "\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hid den size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ed86773-8a3e-4210-f939-75e067e4fa28"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_h[0], sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.num_layers = 8\n",
        "    self.sub_layer = [None]*self.num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      self.sub_layer[i] = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, \n",
        "                                       return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden_hs, hidden_cs, enc_outputs):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    ## Attention calculated from last on query=last sub layer enc_h of encoder \n",
        "    context_vector, attention_weights = self.attention(hidden_hs[self.num_layers-1], enc_outputs)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    dec_hs = [None]*self.num_layers\n",
        "    dec_cs = [None]*self.num_layers\n",
        "    for i in range(self.num_layers):\n",
        "      prev_x = x\n",
        "      x, dec_hs[i] , dec_cs[i] = self.sub_layer[i](x, [hidden_hs[i], hidden_cs[i]])\n",
        "      if(i > 1):\n",
        "        x = prev_x + x   # Residual Connection\n",
        "      \n",
        "        \n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, dec_hs, dec_cs, attention_weights"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6c5cfad4-78e8-4991-81b5-ac1cb1fa9cb4"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, dec_hs, dec_cs, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_h, sample_c, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "print('Decoder_h shape: ', dec_hs[0].shape)\n",
        "print('Decoder_c shape: ', dec_cs[0].shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4936)\n",
            "Decoder_h shape:  (64, 1024)\n",
            "Decoder_c shape:  (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp)\n",
        "    # print(\"here\")\n",
        "\n",
        "    dec_hs = enc_h\n",
        "    dec_cs = enc_c\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hs, dec_cs, _ = decoder(dec_input, dec_hs, dec_cs, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7468a607-f4a9-45a9-c522-ef5da35c16fe"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5272\n",
            "Epoch 1 Batch 100 Loss 0.5730\n",
            "Epoch 1 Batch 200 Loss 0.6099\n",
            "Epoch 1 Batch 300 Loss 0.6561\n",
            "Epoch 1 Loss 0.4801\n",
            "Time taken for 1 epoch 120.6132595539093 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4649\n",
            "Epoch 2 Batch 100 Loss 0.5117\n",
            "Epoch 2 Batch 200 Loss 0.5340\n",
            "Epoch 2 Batch 300 Loss 0.5911\n",
            "Epoch 2 Loss 0.4289\n",
            "Time taken for 1 epoch 136.02092504501343 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4108\n",
            "Epoch 3 Batch 100 Loss 0.4487\n",
            "Epoch 3 Batch 200 Loss 0.4856\n",
            "Epoch 3 Batch 300 Loss 0.5084\n",
            "Epoch 3 Loss 0.3824\n",
            "Time taken for 1 epoch 120.5916314125061 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3674\n",
            "Epoch 4 Batch 100 Loss 0.3970\n",
            "Epoch 4 Batch 200 Loss 0.4439\n",
            "Epoch 4 Batch 300 Loss 0.4721\n",
            "Epoch 4 Loss 0.3388\n",
            "Time taken for 1 epoch 136.07186245918274 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3583\n",
            "Epoch 5 Batch 100 Loss 0.3837\n",
            "Epoch 5 Batch 200 Loss 0.3817\n",
            "Epoch 5 Batch 300 Loss 0.3990\n",
            "Epoch 5 Loss 0.3024\n",
            "Time taken for 1 epoch 120.45960569381714 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2923\n",
            "Epoch 6 Batch 100 Loss 0.3105\n",
            "Epoch 6 Batch 200 Loss 0.3445\n",
            "Epoch 6 Batch 300 Loss 0.3714\n",
            "Epoch 6 Loss 0.2693\n",
            "Time taken for 1 epoch 136.32442378997803 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2697\n",
            "Epoch 7 Batch 100 Loss 0.2751\n",
            "Epoch 7 Batch 200 Loss 0.3192\n",
            "Epoch 7 Batch 300 Loss 0.3536\n",
            "Epoch 7 Loss 0.2373\n",
            "Time taken for 1 epoch 120.44213390350342 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1816\n",
            "Epoch 8 Batch 100 Loss 0.2435\n",
            "Epoch 8 Batch 200 Loss 0.2810\n",
            "Epoch 8 Batch 300 Loss 0.2847\n",
            "Epoch 8 Loss 0.2151\n",
            "Time taken for 1 epoch 136.50350666046143 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1568\n",
            "Epoch 9 Batch 100 Loss 0.2232\n",
            "Epoch 9 Batch 200 Loss 0.2382\n",
            "Epoch 9 Batch 300 Loss 0.2135\n",
            "Epoch 9 Loss 0.1908\n",
            "Time taken for 1 epoch 120.43197464942932 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1894\n",
            "Epoch 10 Batch 100 Loss 0.1906\n",
            "Epoch 10 Batch 200 Loss 0.2346\n",
            "Epoch 10 Batch 300 Loss 0.2443\n",
            "Epoch 10 Loss 0.1700\n",
            "Time taken for 1 epoch 136.216552734375 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_input,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units)), tf.zeros((1,units)), tf.zeros((1, units)), tf.zeros((1,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs)\n",
        "\n",
        "  dec_hs = enc_h\n",
        "  dec_cs = enc_c\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_output):\n",
        "    predictions, dec_hs, dec_cs, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hs, dec_cs,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "793fa68a-0e59-4d3f-b04c-40ac3a5a42fb"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0746a03e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a914826-2fa5-48ff-e9ec-46b968ee8796"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s monday today . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSx2iM36EZQZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01fa4234-644b-41a4-c1ba-2c14f6a52fc6"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3LLCx3ZE0Ls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9fc69c65-3c3f-4c30-80f5-9fd717d8c62a"
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted translation: are you afraid of tom ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DUQVLVqUE1YW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2044f964-4a62-4940-a207-5e0b8ecc23e8"
      },
      "source": [
        "# wrong translation\n",
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: try to figure it out . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92aXUSuvwzOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_batch(test_dataset):\n",
        "  with open('output_text.txt', 'w') as f:\n",
        "    for (inputs, targets) in test_dataset:\n",
        "      outputs = np.zeros((BATCH_SIZE, max_length_output),dtype=np.int16)\n",
        "      # hidden_state = [tf.zeros((BATCH_SIZE, units)), tf.zeros((BATCH_SIZE, units)), tf.zeros((BATCH_SIZE, units)), tf.zeros((BATCH_SIZE, units))] \n",
        "      enc_output, dec_hs, dec_cs = encoder(inputs)\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "      for t in range(max_length_output):\n",
        "        preds, dec_hs, dec_cs,  _ = decoder(dec_input, dec_hs, dec_cs, enc_output)\n",
        "        predicted_id = tf.argmax(preds, axis=1).numpy()\n",
        "        outputs[:, t] = predicted_id\n",
        "        dec_input = tf.expand_dims(predicted_id, 1)\n",
        "      outputs = targ_lang.sequences_to_texts(outputs)\n",
        "      for t, item in enumerate(outputs):\n",
        "        try:\n",
        "          i = item.index('<end>')\n",
        "          f.write(\"%s\\n\" %item[:i])\n",
        "        except: \n",
        "          f.write(\"%s \\n\" % item)\n",
        "\n",
        "outputs = translate_batch(val_dataset)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbHSSD1dT1D0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "448f1db3-a84c-4f78-fc13-2fb66c6a901b"
      },
      "source": [
        "!head output_text.txt\n",
        "! wc -l output_text.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "come see me . \n",
            "i like my family . \n",
            "they gave it . \n",
            "tom died the fence . \n",
            "listen . \n",
            "put it back . \n",
            "let me finish . \n",
            "tom is nervous . \n",
            "he fixed a diary . \n",
            "this one works . \n",
            "5952 output_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij53cfn8exhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}